# **Detailed Design Document: NvM Component**

## **1. Introduction**

### **1.1. Purpose**

This document details the design of the NvM (Non-Volatile Memory) component, which resides in the Service Layer. Its primary purpose is to provide a unified, high-level interface for managing persistent storage of application configuration parameters, calibration data, and other non-volatile information. It abstracts the complexities of underlying Flash and EEPROM (or emulated EEPROM) drivers, providing a block-oriented, wear-leveling-aware mechanism for data storage and retrieval.

### **1.2. Scope**

The scope of this document covers the NvM module's architecture, functional behavior, interfaces, dependencies, and resource considerations. It details how NvM interacts with HAL_FLASH and HAL_EEPROM (or directly with MCAL_FLASH/MCAL_EEPROM if HAL is not used for these) and provides services to Application layer modules (e.g., Application/systemMgr, Application/diagnostic).

### **1.3. References**

* Software Architecture Document (SAD) - Smart Device Firmware (Final Version)  
* Detailed Design Document: HAL_FLASH  
* Detailed Design Document: HAL_EEPROM  
* Application/storage (Conceptual, as NvM would be the underlying service for it)  
* Wear-leveling algorithms (Conceptual)

## **2. Functional Description**

The NvM component provides the following core functionalities:

1. **NvM Initialization**: Initialize the NvM manager and its underlying non-volatile memory drivers.  
2. **Data Block Registration**: Allow application modules to register specific data blocks (e.g., "System Configuration", "Calibration Data") with NvM, including their size, default values, and update frequency.  
3. **Read Data Block**: Retrieve a complete data block from non-volatile memory.  
4. **Write Data Block**: Store a complete data block to non-volatile memory. This operation should ideally be atomic and wear-leveling aware.  
5. **Restore Defaults**: Provide a mechanism to restore a data block to its factory default values.  
6. **Error Reporting**: Report any failures during NvM operations (e.g., initialization failure, read/write error, data corruption, wear-leveling issue) to the SystemMonitor via RTE_SystemMonitor_ReportFault().

## **3. Non-Functional Requirements**

### **3.1. Performance**

* **Read/Write Speed**: NvM operations shall be efficient enough not to impact real-time performance, especially for frequently accessed data.  
* **Wear Leveling**: For Flash-based storage, implement or leverage underlying wear-leveling to maximize the lifetime of the non-volatile memory.

### **3.2. Memory**

* **Minimal Footprint**: The NvM code and data shall have a minimal memory footprint.  
* **Efficient Storage**: Optimize the storage of data blocks to minimize wasted space.

### **3.3. Reliability**

* **Data Integrity**: Ensure data written to NvM is reliably stored and retrieved without corruption, even across power cycles. Implement checksums or CRCs for each data block.  
* **Atomicity**: Write operations for data blocks shall be atomic, meaning either the entire block is successfully written, or the previous valid version remains.  
* **Robustness**: The module shall handle invalid requests or underlying memory failures gracefully.  
* **Fault Isolation**: Failures in NvM operations should be isolated and reported without crashing the system.

## **4. Architectural Context**

As per the SAD (Section 3.1.2, Service Layer), NvM resides in the Service Layer. It acts as a high-level manager for non-volatile storage. It directly interacts with HAL_FLASH and HAL_EEPROM (or their MCAL equivalents) for low-level memory access. Application layer modules (e.g., Application/storage which might wrap NvM for specific application data structures, or Application/systemMgr for its configuration) will use NvM via RTE services.

## **5. Design Details**

### **5.1. Module Structure**

The NvM component will consist of the following files:

* Service/nvm/inc/nvm.h: Public header file containing function prototypes, data types, and error codes.  
* Service/nvm/src/nvm.c: Source file containing the implementation of the NvM functions.  
* Service/nvm/cfg/nvm_cfg.h: Configuration header for defining NvM data blocks, their sizes, and memory allocation.

### **5.2. Public Interface (API)**

// In Service/nvm/inc/nvm.h
```c
// Enum for NvM status/error codes  
typedef enum {  
    NVM_OK = 0,  
    NVM_ERROR_INIT_FAILED,  
    NVM_ERROR_READ_FAILED,  
    NVM_ERROR_WRITE_FAILED,  
    NVM_ERROR_INVALID_BLOCK_ID,  
    NVM_ERROR_DATA_CORRUPT,  
    NVM_ERROR_BUFFER_TOO_SMALL,  
    // Add more specific errors as needed  
} NVM_Status_t;

// Enum for NvM block IDs (logical identifiers for each data block)  
typedef enum {  
    NVM_BLOCK_ID_SYS_CONFIG,        // System operational parameters  
    NVM_BLOCK_ID_CALIBRATION_DATA,  // Sensor calibration values  
    NVM_BLOCK_ID_FAULT_LOG,         // Persistent fault log  
    // Add more block IDs as needed  
    NVM_BLOCK_ID_COUNT  
} NVM_BlockId_t;

// Structure to define an NvM data block  
typedef struct {  
    NVM_BlockId_t block_id;  
    uint32_t size_bytes;             // Size of the data block in bytes  
    const uint8_t *default_data;     // Pointer to default data (in Flash/ROM)  
    // Add flags for wear-leveling strategy, target memory (Flash/EEPROM)  
} NVM_BlockConfig_t;

/**  
 * @brief Initializes the NvM module and its underlying memory drivers.  
 * This function should be called once during system initialization.  
 * It will also perform initial checks on registered data blocks.  
 * @return NVM_OK on success, an error code on failure.  
 */  
NVM_Status_t NVM_Init(void);

/**  
 * @brief Reads a data block from non-volatile memory.  
 * @param block_id The ID of the data block to read.  
 * @param buffer Pointer to the buffer where the data will be copied.  
 * @param buffer_len Size of the provided buffer.  
 * @return NVM_OK on success, an error code on failure.  
 */  
NVM_Status_t NVM_ReadBlock(NVM_BlockId_t block_id,  
                                           uint8_t *buffer, uint32_t buffer_len);

/**  
 * @brief Writes a data block to non-volatile memory.  
 * This operation is atomic and wear-leveling aware.  
 * @param block_id The ID of the data block to write.  
 * @param data Pointer to the data to write.  
 * @param data_len Length of the data to write.  
 * @return NVM_OK on success, an error code on failure.  
 */  
NVM_Status_t NVM_WriteBlock(NVM_BlockId_t block_id,  
                                            const uint8_t *data, uint32_t data_len);

/**  
 * @brief Restores a data block to its factory default values.  
 * @param block_id The ID of the data block to restore.  
 * @return NVM_OK on success, an error code on failure.  
 */  
NVM_Status_t NVM_RestoreDefaults(NVM_BlockId_t block_id);

/**  
 * @brief Gets the configured size of a data block.  
 * @param block_id The ID of the data block.  
 * @param size Pointer to store the size in bytes.  
 * @return NVM_OK on success, NVM_ERROR_INVALID_BLOCK_ID if not found.  
 */  
NVM_Status_t NVM_GetBlockSize(NVM_BlockId_t block_id, uint32_t *size);
```

### **5.3. Internal Design**

The NvM module will manage the mapping of logical data blocks to physical memory locations (in Flash or EEPROM). It will implement or leverage underlying wear-leveling strategies and ensure data integrity using checksums/CRCs.

1. **Initialization (NVM_Init)**:  
   * Initialize HAL_FLASH_Init() and HAL_EEPROM_Init(). If any fail, report NVM_ERROR_INIT_FAILED to SystemMonitor.  
   * Loop through the nvm_block_configs array (defined in nvm_cfg.h).  
   * For each registered block:  
     * Determine its physical storage location (Flash or EEPROM) based on configuration.  
     * Read the block from memory.  
     * Verify its integrity (e.g., checksum/CRC).  
     * If corrupted or not found, write the default data to the block.  
     * Report NVM_ERROR_DATA_CORRUPT or NVM_ERROR_READ_FAILED to SystemMonitor if issues are found.  
   * Return NVM_OK.  
2. **Data Block Management (Internal)**:  
   * NvM will maintain an internal table or data structure that maps NVM_BlockId_t to its physical address, size, and other metadata.  
   * **Wear-Leveling Strategy**:  
     * For Flash-based blocks: Implement a simple wear-leveling scheme. This could involve writing new versions of a block to a different physical location within a designated Flash "page" or "sector" pool. A header or index would point to the latest valid version. When the pool is full, a garbage collection/defragmentation process might be triggered, erasing old data and compacting valid data. This requires careful management of Flash erase blocks.  
     * For EEPROM-based blocks: EEPROM typically handles wear-leveling internally for byte writes, but for block writes, NvM might still use a similar "write to next available slot" approach if the underlying HAL_EEPROM does not provide sufficient wear-leveling for block writes.  
   * **Data Integrity**: Each stored data block will include a checksum (e.g., CRC16, CRC32) or a simple integrity check. This checksum will be calculated before writing and verified after reading.  
3. **Read Block (NVM_ReadBlock)**:  
   * Validate block_id and buffer_len (ensure buffer is large enough).  
   * Look up the block's physical location and size from the internal table.  
   * Call HAL_FLASH_Read() or HAL_EEPROM_ReadBlock() to read the raw data.  
   * Calculate the checksum of the read data and compare it with the stored checksum.  
   * If checksum mismatch or read error, report NVM_ERROR_DATA_CORRUPT or NVM_ERROR_READ_FAILED to SystemMonitor and return error.  
   * Copy the valid data (excluding checksum) to the user-provided buffer.  
4. **Write Block (NVM_WriteBlock)**:  
   * Validate block_id, data, and data_len. Ensure data_len matches the registered block size.  
   * Calculate the checksum of the data.  
   * Prepare the data for writing, including the checksum.  
   * Determine the next available physical location for the block (based on wear-leveling strategy).  
   * If the new location requires erasing a Flash sector, call HAL_FLASH_EraseSector().  
   * Call HAL_FLASH_Write() or HAL_EEPROM_WriteBlock() to write the data and checksum.  
   * This operation should be atomic: write the new block, then update a pointer/index to point to the new block. If power fails during write, the old block remains valid.  
   * If write fails, report NVM_ERROR_WRITE_FAILED to SystemMonitor.  
5. **Restore Defaults (NVM_RestoreDefaults)**:  
   * Validate block_id.  
   * Retrieve the default_data pointer from the block's configuration.  
   * Call NVM_WriteBlock() with the default data.

**Sequence Diagram (Example: Application Reads System Configuration):**
```mermaid
sequenceDiagram  
    participant AppLayer as Application Layer (e.g., systemMgr)  
    participant RTE as Runtime Environment  
    participant NvM as Service/NvM  
    participant HAL_FLASH as HAL/FLASH  
    participant SystemMonitor as Application/SystemMonitor

    AppLayer->>RTE: RTE_NVM_ReadBlock(NVM_BLOCK_ID_SYS_CONFIG, &config_buffer, sizeof(config_buffer))  
    RTE->>NvM: NVM_ReadBlock(NVM_BLOCK_ID_SYS_CONFIG, &config_buffer, sizeof(config_buffer))  
    NvM->>NvM: Validate parameters, lookup block config  
    NvM->>HAL_FLASH: HAL_FLASH_Read(physical_addr, raw_data_buffer, block_size_with_checksum)  
    alt HAL_FLASH_Read returns ERROR  
        HAL_FLASH--xNvM: Return ERROR  
        NvM->>SystemMonitor: RTE_SystemMonitor_ReportFault(NVM_READ_FAILED, SEVERITY_HIGH, ...)  
        NvM--xRTE: Return NVM_ERROR_READ_FAILED  
        RTE--xAppLayer: Return APP_ERROR  
    else HAL_FLASH_Read returns OK  
        HAL_FLASH-->>NvM: Return OK (raw_data_buffer)  
        NvM->>NvM: Calculate checksum of raw_data_buffer  
        NvM->>NvM: Compare calculated checksum with stored checksum  
        alt Checksum Mismatch  
            NvM->>SystemMonitor: RTE_SystemMonitor_ReportFault(NVM_DATA_CORRUPT, SEVERITY_CRITICAL, ...)  
            NvM--xRTE: Return NVM_ERROR_DATA_CORRUPT  
            RTE--xAppLayer: Return APP_ERROR  
        else Checksum Match  
            NvM->>NvM: Copy valid data (excluding checksum) to config_buffer  
            NvM-->>RTE: Return NVM_OK  
            RTE-->>AppLayer: Return APP_OK  
        end  
    end
```

### **5.4. Dependencies**

* **HAL/inc/hal_flash.h**: For Flash memory access.  
* **HAL/inc/hal_eeprom.h**: For EEPROM memory access (if used).  
* **Application/logger/inc/logger.h**: For internal logging.  
* **Rte/inc/Rte.h**: For calling RTE_SystemMonitor_ReportFault().  
* **Application/common/inc/app_common.h**: For APP_Status_t.  
* **Service/nvm/cfg/nvm_cfg.h**: For NvM block definitions.

### **5.5. Error Handling**

* **Input Validation**: All public API functions will validate input parameters (e.g., valid block_id, non-NULL buffers, correct buffer lengths).  
* **Underlying Driver Error Propagation**: Errors returned by HAL_FLASH or HAL_EEPROM will be caught by NvM.  
* **Data Integrity Check**: Checksums/CRCs are used to verify data integrity upon reading.  
* **Fault Reporting**: Upon detection of an error (invalid input, underlying memory driver failure, data corruption), NvM will report a specific fault ID (e.g., NVM_ERROR_INIT_FAILED, NVM_ERROR_READ_FAILED, NVM_ERROR_WRITE_FAILED, NVM_ERROR_DATA_CORRUPT) to SystemMonitor via the RTE service.  
* **Return Status**: All public API functions will return NVM_Status_t indicating success or specific error.

### **5.6. Configuration**

The Service/nvm/cfg/nvm_cfg.h file will contain:

* The nvm_block_configs array, defining each NvM data block:  
  * block_id: Unique identifier.  
  * size_bytes: Expected size of the data structure.  
  * default_data: Pointer to a constant array in Flash/ROM containing the factory default values for this block.  
  * storage_type: (Optional) Enum indicating whether this block should be stored in Flash or EEPROM (if both are available).  
  * wear_leveling_group: (Optional) Identifier for wear-leveling groups if different blocks share wear-leveling pools.  
* Total NvM storage size allocated in Flash/EEPROM.

// Example: Service/nvm/cfg/nvm_cfg.h
```c
// Forward declaration of default data arrays  
extern const uint8_t sys_config_default_data[];  
extern const uint8_t calibration_default_data[];

// NvM Block Configurations  
const NVM_BlockConfig_t nvm_block_configs[] = {  
    {  
        .block_id = NVM_BLOCK_ID_SYS_CONFIG,  
        .size_bytes = sizeof(SystemConfig_t), // Assuming SystemConfig_t is defined elsewhere  
        .default_data = sys_config_default_data,  
        // .storage_type = NVM_STORAGE_TYPE_FLASH, // Example  
    },  
    {  
        .block_id = NVM_BLOCK_ID_CALIBRATION_DATA,  
        .size_bytes = sizeof(CalibrationData_t), // Assuming CalibrationData_t is defined elsewhere  
        .default_data = calibration_default_data,  
        // .storage_type = NVM_STORAGE_TYPE_EEPROM, // Example  
    },  
    // Add more blocks as needed  
};

const uint32_t nvm_block_configs_count = sizeof(nvm_block_configs) / sizeof(NVM_BlockConfig_t);

// Define default data content (in a .c file, but declared here)  
// const uint8_t sys_config_default_data[] = { /* ... default bytes ... */ };
```

### **5.7. Resource Usage**

* **Flash**: Moderate, for the NvM driver code, configuration table, and default data arrays. Additionally, requires dedicated Flash sectors/pages for actual data storage.  
* **RAM**: Moderate, for internal state, block metadata, and temporary buffers for read/write operations (especially for wear-leveling).  
* **CPU**: Low for read operations. Moderate to high for write operations, particularly if Flash erase/write cycles or wear-leveling garbage collection are involved.

## **6. Test Considerations**

### **6.1. Unit Testing**

* **Mock Dependencies**: Unit tests for NvM will mock HAL_FLASH and HAL_EEPROM functions to isolate NvM's logic.  
* **Test Cases**:  
  * NVM_Init: Test with valid/invalid configurations. Mock underlying memory drivers to simulate success/failure. Verify initial data integrity checks and default data restoration.  
  * NVM_ReadBlock: Test valid/invalid block_id, buffer sizes. Mock underlying reads to simulate success, read errors, and data corruption (checksum mismatch). Verify correct data retrieval and error reporting.  
  * NVM_WriteBlock: Test valid/invalid block_id, data, lengths. Mock underlying writes to simulate success, write errors, and wear-leveling behavior (if implemented internally). Verify atomicity (by checking if old data is preserved on write failure).  
  * NVM_RestoreDefaults: Verify that the correct default data is written.  
  * NVM_GetBlockSize: Test valid/invalid block_id.  
  * Error reporting: Verify that RTE_SystemMonitor_ReportFault() is called with the correct fault ID on various error conditions.

### **6.2. Integration Testing**

* **NvM-HAL Integration**: Verify that NvM correctly interfaces with the actual HAL_FLASH and HAL_EEPROM drivers and the physical non-volatile memory.  
* **Data Persistence**: Write data blocks, power cycle the device, and then read the data to verify persistence.  
* **Wear Leveling**: For Flash-based NvM, perform a large number of write cycles to a single logical block and monitor the physical write locations to verify wear-leveling is active and effective.  
* **Data Corruption Simulation**: Intentionally corrupt data in Flash/EEPROM (e.g., by directly writing garbage to memory using a debugger) and verify that NvM detects data corruption and potentially restores defaults.  
* **Power Loss during Write**: Simulate power loss during a write operation and verify that the data block remains in a consistent state (either the old valid data or the new valid data, but not a corrupted mix).  
* **Boundary Conditions**: Test reading/writing at the beginning and end of NvM blocks and the overall memory region.

### **6.3. System Testing**

* **Application Configuration Persistence**: Verify that application configurations (e.g., operational ranges, schedules) saved by Application/storage (which uses NvM) persist across reboots and power cycles.  
* **Fault Log Persistence**: Verify that the fault log (if stored in NvM) persists and can be retrieved after reboots.  
* **OTA Update Resilience**: Ensure that NvM operations are not corrupted or negatively impacted during OTA updates (if OTA writes to Flash regions near NvM).  
* **Long-Term Reliability**: Run the system for extended periods with frequent NvM access to detect any long-term issues related to wear-leveling or data integrity.
